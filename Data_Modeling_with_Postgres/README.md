# Data Modeling with Postgres


## Overview

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Currently, these data reside in JSON files.

In order to optimize queries on song play analysis, in this project, we will design a database and build ETL pipeline to move data from JSON file to Postgres database.
                                                

## DataSets


#### Song Datasets
 - A subset of real data from the Million Song Dataset website; 
 - Each file is JSON format and contains metadata and artist of that song. 
 - Example of song file:

``` 
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```
#### Log Datasets
 - Log files is generated by an [event simulator](https://github.com/Interana/eventsim), with JSON format 
 - Example of log file
```
{"artist":"Line Renaud", "auth":"Logged In", "firstName":"Jayden", "gender":"M", "itemInSession":0, "lastName":"Fox", "length":152.92036, "level":"free", "location":"New Orleans-Metairie,LA", "method":"PUT", "page":"NextSong", "registration":1541033612796.0, "sessionId":184,"song":"Der Kleine Dompfaff", "status":200, "ts":1541121934796, "userAgent":"\"Mozilla\/5.0 ","userId":"101"}
```


## Schema

Based on the song metadata and log data, and the analytics team's reqirement, design the fact and dimension tables for a star schema. 

#### Fact Table:
1. **songplays** - records in log data associated with song plays i.e. records with page NextSong
 - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Table:
2. **users** - users in the app
 - user_id, first_name, last_name, gender, level

3. **songs** - songs in music database
 - song_id, title, artist_id, year, duration

4. **artists** - artists in music database
 - artist_id, name, location, latitude, longitude

5. **time** - timestamps of records in songplays broken down into specific units
 - start_time, hour, day, week, month, year, weekday



## Build ETL Pipeline

- First, perform ETL on the song datasets to create the songs and artists dimensional tables.   
- Second, perform ETL on the log_data dataset to create the time and users dimensional tables, as well as the songplays fact table.
  
- Python libraries used during ETL 
    - Extract Data from JSON file: using pandas dataframe to read JSON file; 
    
    ```
    import pandas as pd
    df = pd.to_read(filepath, lines="True")
    ```
    
    - Insert data into Postgres Database: using psycopg2
    
    ```
    import psyconpg2
    conn = psyconpg2.connection()
    cur = conn.cursor()
    cur.execute(query)
    ```


## Project File Structure


- `sql_queries.py` : contains all the sql queries 
- `create_tables.py` : drops and creates tables
- `etl.py` : reads and processes song and log data files and loads them into tables
- `etl.ipynb` : a notebook to develop the ETL process for each of your tables 
- `test.ipynb` : displays the first few rows of each table to let you check your database
- `README.md` :  provides discussion on your project.



## How to Run

- **Environment**: Python 3.6.3  and PostgreSQL 9.5.21
- In the console, run the following command to create database and fact and dimension tables:

```
python create_tables.py
```
- In the console, run the following command to load data from JSON files to database:

```
python etl.py
```



 
